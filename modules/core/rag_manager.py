"""
RAG (Retrieval-Augmented Generation) ê´€ë¦¬ ëª¨ë“ˆ

FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ OCR í…ìŠ¤íŠ¸ì™€ ì •ë‹µ JSON ìŒì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
"""

import os
import json
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from threading import Lock
import faiss


class RAGManager:
    """
    RAG ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤
    
    FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ OCR í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    
    # í´ë˜ìŠ¤ ë ˆë²¨ ë½ (ëª¨ë¸ ë¡œë”© ë™ê¸°í™”ìš©)
    _model_lock = Lock()
    
    def __init__(self, persist_directory: Optional[str] = None, use_db: bool = True):
        """
        RAG Manager ì´ˆê¸°í™”
        
        Args:
            persist_directory: ë²¡í„° DB ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸/rag_db, use_db=Trueì¼ ë•ŒëŠ” ì‚¬ìš© ì•ˆ í•¨)
            use_db: Trueë©´ DBì— ì €ì¥, Falseë©´ ë¡œì»¬ íŒŒì¼ì— ì €ì¥ (ê¸°ë³¸ê°’: True)
        """
        self.use_db = use_db
        
        if persist_directory is None:
            from modules.utils.config import get_project_root
            project_root = get_project_root()
            persist_directory = str(project_root / "rag_db")
        
        self.persist_directory = persist_directory
        
        # DB ì—°ê²° (use_db=Trueì¼ ë•Œë§Œ)
        if self.use_db:
            from database.registry import get_db
            import psycopg2
            self.db = get_db()
            self._ensure_vector_index_table_exists()  # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        else:
            # ë¡œì»¬ íŒŒì¼ ëª¨ë“œ: ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ì„¤ì •
            os.makedirs(persist_directory, exist_ok=True, mode=0o755)
            
            # íŒŒì¼ ê²½ë¡œ (rag_db êµ¬ì¡°)
            self.base_index_path = os.path.join(persist_directory, "base.faiss")
            self.base_metadata_path = os.path.join(persist_directory, "base_metadata.json")
            self.index_path = self.base_index_path
            self.metadata_path = self.base_metadata_path
        
        # shard ë””ë ‰í† ë¦¬ (íŒŒì¼ ëª¨ë“œ fallbackì„ ìœ„í•´ í•­ìƒ ì´ˆê¸°í™”)
        self.shards_dir = os.path.join(persist_directory, "shards")
        if not self.use_db:
            os.makedirs(self.shards_dir, exist_ok=True, mode=0o755)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self._embedding_model = None
        
        # FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.index = None
        self.metadata = {}  # {doc_id: {ocr_text, answer_json, metadata}}
        self.id_to_index = {}  # {doc_id: faiss_index}
        self.index_to_id = {}  # {faiss_index: doc_id}
        self._load_index()
        
        # BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self._bm25_index = None
        self._bm25_texts = None
        self._bm25_example_map = None
    
    def _get_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì§€ì—° ë¡œë”©, ìŠ¤ë ˆë“œ ì•ˆì „)"""
        # ì´ì¤‘ ì²´í¬ ë½í‚¹ íŒ¨í„´ ì‚¬ìš©
        if self._embedding_model is None:
            with RAGManager._model_lock:  # í´ë˜ìŠ¤ ë ˆë²¨ ë½ ì‚¬ìš©
                # ë‹¤ì‹œ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ì´ë¯¸ ë¡œë“œí–ˆì„ ìˆ˜ ìˆìŒ)
                if self._embedding_model is None:
                    try:
                        # tokenizers ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë°©ì§€ (ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ì—ì„œ ì•ˆì „)
                        # ëª¨ë¸ ë¡œë”© ì „ì— ì„¤ì •
                        os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
                        
                        from sentence_transformers import SentenceTransformer
                        # ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš© (ì¼ë³¸ì–´/í•œêµ­ì–´/ì˜ì–´ ì§€ì›)
                        # device íŒŒë¼ë¯¸í„° ì œê±° - sentence-transformersê°€ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì„ íƒ
                        # ëª…ì‹œì  device ì„¤ì •ì€ ë©”íƒ€ í…ì„œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŒ
                        self._embedding_model = SentenceTransformer(
                            'paraphrase-multilingual-MiniLM-L12-v2'
                            # device íŒŒë¼ë¯¸í„° ì œê±° - ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ
                        )
                    except ImportError:
                        raise ImportError(
                            "sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                            "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install sentence-transformers"
                        )
        return self._embedding_model
    
    def _get_embedding_dim(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        model = self._get_embedding_model()
        # í…ŒìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ì°¨ì› í™•ì¸
        test_embedding = model.encode(["test"], convert_to_numpy=True)
        return test_embedding.shape[1]
    
    def _load_index(self):
        """FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ (DB ë˜ëŠ” íŒŒì¼)"""
        embedding_dim = self._get_embedding_dim()
        
        if self.use_db:
            # DBì—ì„œ ë¡œë“œ
            self.index, self.metadata, self.id_to_index, self.index_to_id = self._load_index_from_db()
            
            # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            if self.index is None:
                self.index = faiss.IndexFlatL2(embedding_dim)
                self.metadata = {}
                self.id_to_index = {}
                self.index_to_id = {}
        else:
            # íŒŒì¼ì—ì„œ ë¡œë“œ (ê¸°ì¡´ ë°©ì‹)
            if os.path.exists(self.base_index_path):
                try:
                    self.index = faiss.read_index(self.base_index_path)
                except Exception as e:
                    print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")
                    self.index = faiss.IndexFlatL2(embedding_dim)
            else:
                self.index = faiss.IndexFlatL2(embedding_dim)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            self.metadata, self.id_to_index, self.index_to_id = self._load_metadata_from_file(self.metadata_path)
        
        # index_to_id ë§¤í•‘ì´ ë¶ˆì™„ì „í•˜ë©´ id_to_indexë¡œ ì¬êµ¬ì¶•
        if len(self.index_to_id) < len(self.id_to_index):
            print(f"âš ï¸ index_to_id ë§¤í•‘ ë¶ˆì™„ì „, ì¬êµ¬ì¶• ì¤‘... ({len(self.index_to_id)}/{len(self.id_to_index)})")
            self.index_to_id = {idx: doc_id for doc_id, idx in self.id_to_index.items()}
            self._save_index()  # ì¬êµ¬ì¶•ëœ ë§¤í•‘ ì €ì¥
    
    def _ensure_vector_index_table_exists(self):
        """rag_vector_index í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'rag_vector_index'
                    )
                """)
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    print("ğŸ“‹ RAG ë²¡í„° ì¸ë±ìŠ¤ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ìƒì„± ì¤‘...")
                    # rag_vector_index í…Œì´ë¸” ìƒì„±
                    cursor.execute("""
                        CREATE TABLE rag_vector_index (
                            index_id SERIAL PRIMARY KEY,
                            index_name VARCHAR(100) NOT NULL DEFAULT 'base',
                            index_data BYTEA NOT NULL,
                            metadata_json JSONB NOT NULL,
                            index_size BIGINT,
                            vector_count INTEGER DEFAULT 0,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(index_name)
                        )
                    """)
                    # ì¸ë±ìŠ¤ ìƒì„±
                    cursor.execute("""
                        CREATE INDEX idx_rag_vector_index_name 
                        ON rag_vector_index(index_name)
                    """)
                    print("âœ… RAG ë²¡í„° ì¸ë±ìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰ (í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ)
    
    def _load_index_from_db(self, exclude_shard_name: Optional[str] = None) -> Tuple[Optional[Any], Dict[str, Any], Dict[str, int], Dict[int, str]]:
        """
        DBì—ì„œ FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ (base ìš°ì„ , ì—†ìœ¼ë©´ shard ë³‘í•©)
        
        Args:
            exclude_shard_name: ë³‘í•©ì—ì„œ ì œì™¸í•  shard ì´ë¦„ (merge_shardì—ì„œ ì‚¬ìš©)
        
        Returns:
            (index, metadata, id_to_index, index_to_id) íŠœí”Œ
        """
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                # 1. base ì¸ë±ìŠ¤ ìš°ì„  ì‹œë„
                cursor.execute("""
                    SELECT index_data, metadata_json, vector_count
                    FROM rag_vector_index
                    WHERE index_name = 'base'
                    ORDER BY updated_at DESC
                    LIMIT 1
                """)
                
                row = cursor.fetchone()
                if row:
                    index_data_bytes = row[0]  # BYTEA (memoryviewì¼ ìˆ˜ ìˆìŒ)
                    metadata_json = row[1]  # JSONB
                    vector_count = row[2] or 0
                    
                    # BYTEAë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (faiss.deserialize_indexëŠ” numpy ë°°ì—´ í•„ìš”)
                    if isinstance(index_data_bytes, memoryview):
                        index_data_bytes = np.frombuffer(index_data_bytes, dtype=np.uint8)
                    elif isinstance(index_data_bytes, bytes):
                        index_data_bytes = np.frombuffer(index_data_bytes, dtype=np.uint8)
                    else:
                        index_data_bytes = np.frombuffer(bytes(index_data_bytes), dtype=np.uint8)
                    
                    # FAISS ì¸ë±ìŠ¤ë¥¼ ë°”ì´íŠ¸ì—ì„œ ë¡œë“œ
                    index = faiss.deserialize_index(index_data_bytes)
                    
                    # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                    metadata = metadata_json.get('metadata', {})
                    id_to_index = metadata_json.get('id_to_index', {})
                    index_to_id_raw = metadata_json.get('index_to_id', {})
                    # JSONì—ì„œ ë¡œë“œí•˜ë©´ í‚¤ê°€ ë¬¸ìì—´ì´ë¯€ë¡œ ì •ìˆ˜ë¡œ ë³€í™˜
                    index_to_id = {int(k): v for k, v in index_to_id_raw.items()}
                    
                    print(f"âœ… DBì—ì„œ base ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({vector_count}ê°œ ë²¡í„°)")
                    return index, metadata, id_to_index, index_to_id
                
                # 2. baseê°€ ì—†ìœ¼ë©´ ëª¨ë“  shardë¥¼ ë³‘í•©í•˜ì—¬ ë¡œë“œ (ì œì™¸í•  shard ì œì™¸)
                if exclude_shard_name:
                    cursor.execute("""
                        SELECT index_data, metadata_json, vector_count, index_name
                        FROM rag_vector_index
                        WHERE index_name LIKE 'shard_%' AND index_name != %s
                        ORDER BY updated_at DESC
                    """, (exclude_shard_name,))
                else:
                    cursor.execute("""
                        SELECT index_data, metadata_json, vector_count, index_name
                        FROM rag_vector_index
                        WHERE index_name LIKE 'shard_%'
                        ORDER BY updated_at DESC
                    """)
                
                shard_rows = cursor.fetchall()
                if not shard_rows:
                    return None, {}, {}, {}
                
                # ì²« ë²ˆì§¸ shardë¥¼ baseë¡œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ë³‘í•©
                if not shard_rows:
                    return None, {}, {}, {}
                
                embedding_dim = self._get_embedding_dim()
                first_shard_data, first_metadata_json, first_vector_count, first_shard_name = shard_rows[0]
                
                # ì²« ë²ˆì§¸ shardë¥¼ base ì¸ë±ìŠ¤ë¡œ ë¡œë“œ
                # BYTEAë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                if isinstance(first_shard_data, memoryview):
                    first_shard_data = np.frombuffer(first_shard_data, dtype=np.uint8)
                elif isinstance(first_shard_data, bytes):
                    first_shard_data = np.frombuffer(first_shard_data, dtype=np.uint8)
                else:
                    first_shard_data = np.frombuffer(bytes(first_shard_data), dtype=np.uint8)
                
                base_index = faiss.deserialize_index(first_shard_data)
                base_metadata = first_metadata_json.get('metadata', {})
                base_id_to_index = first_metadata_json.get('id_to_index', {})
                base_index_to_id_raw = first_metadata_json.get('index_to_id', {})
                base_index_to_id = {int(k): v for k, v in base_index_to_id_raw.items()}
                
                # ë‚˜ë¨¸ì§€ shardë“¤ì„ ë³‘í•©
                for shard_data_bytes, shard_metadata_json, shard_vector_count, shard_name in shard_rows[1:]:
                    # BYTEAë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                    if isinstance(shard_data_bytes, memoryview):
                        shard_data_bytes = np.frombuffer(shard_data_bytes, dtype=np.uint8)
                    elif isinstance(shard_data_bytes, bytes):
                        shard_data_bytes = np.frombuffer(shard_data_bytes, dtype=np.uint8)
                    else:
                        shard_data_bytes = np.frombuffer(bytes(shard_data_bytes), dtype=np.uint8)
                    
                    shard_index = faiss.deserialize_index(shard_data_bytes)
                    base_vector_count = base_index.ntotal
                    
                    # FAISS ì¸ë±ìŠ¤ ë³‘í•©
                    base_index.merge_from(shard_index)
                    
                    # ë©”íƒ€ë°ì´í„° ë³‘í•© (ì¸ë±ìŠ¤ ì˜¤í”„ì…‹ ì¡°ì •)
                    shard_metadata = shard_metadata_json.get('metadata', {})
                    shard_id_to_index = shard_metadata_json.get('id_to_index', {})
                    shard_index_to_id_raw = shard_metadata_json.get('index_to_id', {})
                    shard_index_to_id = {int(k): v for k, v in shard_index_to_id_raw.items()}
                    
                    for doc_id, shard_faiss_idx in shard_id_to_index.items():
                        new_faiss_idx = base_vector_count + shard_faiss_idx
                        base_metadata[doc_id] = shard_metadata.get(doc_id, {})
                        base_id_to_index[doc_id] = new_faiss_idx
                        base_index_to_id[new_faiss_idx] = doc_id
                
                total_vectors = base_index.ntotal
                print(f"âœ… DBì—ì„œ shard ë³‘í•©í•˜ì—¬ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(shard_rows)}ê°œ shard, {total_vectors}ê°œ ë²¡í„°)")
                
                # shardë¥¼ ë³‘í•©í•œ ì¸ë±ìŠ¤ë¥¼ baseë¡œ ì €ì¥ (ë‹¤ìŒ ë¡œë“œ ì‹œ ë¹ ë¥´ê²Œ ë¡œë“œ)
                try:
                    self._save_merged_index_to_db(base_index, base_metadata, base_id_to_index, base_index_to_id, total_vectors)
                    print(f"ğŸ’¾ ë³‘í•©ëœ ì¸ë±ìŠ¤ë¥¼ baseë¡œ ì €ì¥ ì™„ë£Œ (ë‹¤ìŒ ë¡œë“œ ì‹œ ë¹ ë¥¸ ë¡œë“œ)")
                except Exception as save_err:
                    print(f"âš ï¸ base ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì‚¬ìš© ê°€ëŠ¥): {save_err}")
                
                return base_index, base_metadata, base_id_to_index, base_index_to_id
                
        except Exception as e:
            print(f"âš ï¸ DBì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None, {}, {}, {}
    
    def _load_metadata_from_file(self, metadata_path: str) -> Tuple[Dict[str, Any], Dict[str, int], Dict[int, str]]:
        """
        ë©”íƒ€ë°ì´í„° íŒŒì¼ì—ì„œ ë¡œë“œ (í—¬í¼ ë©”ì„œë“œ)
        
        Args:
            metadata_path: ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (metadata, id_to_index, index_to_id) íŠœí”Œ
        """
        if not os.path.exists(metadata_path):
            return {}, {}, {}
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                metadata = data.get("metadata", {})
                id_to_index = data.get("id_to_index", {})
                index_to_id_raw = data.get("index_to_id", {})
                # JSONì—ì„œ ë¡œë“œí•˜ë©´ í‚¤ê°€ ë¬¸ìì—´ì´ë¯€ë¡œ ì •ìˆ˜ë¡œ ë³€í™˜
                index_to_id = {int(k): v for k, v in index_to_id_raw.items()}
                return metadata, id_to_index, index_to_id
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}, {}, {}
    
    def _save_index(self):
        """FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ (DB ë˜ëŠ” íŒŒì¼)"""
        try:
            if self.use_db:
                # DBì— ì €ì¥
                self._save_index_to_db()
            else:
                # íŒŒì¼ì— ì €ì¥ (ê¸°ì¡´ ë°©ì‹)
                faiss.write_index(self.index, self.base_index_path)
                
                # base ë©”íƒ€ë°ì´í„° ì €ì¥
                data = {
                    "metadata": self.metadata,
                    "id_to_index": self.id_to_index,
                    "index_to_id": self.index_to_id
                }
                with open(self.base_metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                # ê²½ë¡œ ì—…ë°ì´íŠ¸
                self.index_path = self.base_index_path
                self.metadata_path = self.base_metadata_path
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_merged_index_to_db(
        self, 
        index: Any, 
        metadata: Dict[str, Any], 
        id_to_index: Dict[str, int], 
        index_to_id: Dict[int, str],
        vector_count: int
    ):
        """ë³‘í•©ëœ ì¸ë±ìŠ¤ë¥¼ DBì— baseë¡œ ì €ì¥ (ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ)"""
        try:
            # FAISS ì¸ë±ìŠ¤ë¥¼ ë°”ì´íŠ¸ë¡œ ì§ë ¬í™”
            serialized = faiss.serialize_index(index)
            if hasattr(serialized, 'tobytes'):
                index_data_bytes = serialized.tobytes()
            else:
                index_data_bytes = bytes(serialized)
            index_size = len(index_data_bytes)
            
            # NaN ê°’ ì²˜ë¦¬
            def clean_for_json(obj):
                """NaN, Infinity ê°’ì„ nullë¡œ ë³€í™˜"""
                import math
                if isinstance(obj, dict):
                    return {k: clean_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_for_json(item) for item in obj]
                elif isinstance(obj, float):
                    if math.isnan(obj) or math.isinf(obj):
                        return None
                    return obj
                return obj
            
            cleaned_metadata = clean_for_json(metadata)
            metadata_json = {
                "metadata": cleaned_metadata,
                "id_to_index": id_to_index,
                "index_to_id": {str(k): v for k, v in index_to_id.items()}
            }
            
            # DBì— ì €ì¥
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO rag_vector_index (
                        index_name, index_data, metadata_json, index_size, vector_count
                    ) VALUES (%s, %s, %s::jsonb, %s, %s)
                    ON CONFLICT (index_name)
                    DO UPDATE SET
                        index_data = EXCLUDED.index_data,
                        metadata_json = EXCLUDED.metadata_json,
                        index_size = EXCLUDED.index_size,
                        vector_count = EXCLUDED.vector_count,
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    'base',
                    index_data_bytes,
                    json.dumps(metadata_json, allow_nan=False),
                    index_size,
                    vector_count
                ))
        except Exception as e:
            print(f"âš ï¸ ë³‘í•© ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _save_index_to_db(self):
        """DBì— FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            # FAISS ì¸ë±ìŠ¤ë¥¼ ë°”ì´íŠ¸ë¡œ ì§ë ¬í™”
            # serialize_indexëŠ” bytesë¥¼ ë°˜í™˜í•˜ì§€ë§Œ, psycopg2 í˜¸í™˜ì„ ìœ„í•´ ëª…ì‹œì  ë³€í™˜
            serialized = faiss.serialize_index(self.index)
            # numpy ë°°ì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ bytesë¡œ ë³€í™˜
            if hasattr(serialized, 'tobytes'):
                index_data_bytes = serialized.tobytes()
            else:
                index_data_bytes = bytes(serialized)
            index_size = len(index_data_bytes)
            vector_count = self.index.ntotal if self.index else 0
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (NaN ê°’ ì²˜ë¦¬)
            def clean_for_json(obj):
                """NaN, Infinity ê°’ì„ nullë¡œ ë³€í™˜"""
                import math
                if isinstance(obj, dict):
                    return {k: clean_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_for_json(item) for item in obj]
                elif isinstance(obj, float):
                    if math.isnan(obj) or math.isinf(obj):
                        return None
                    return obj
                return obj
            
            cleaned_metadata = clean_for_json(self.metadata)
            metadata_json = {
                "metadata": cleaned_metadata,
                "id_to_index": self.id_to_index,
                "index_to_id": {str(k): v for k, v in self.index_to_id.items()}  # í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            }
            
            # DBì— ì €ì¥ (UPSERT)
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO rag_vector_index (
                        index_name, index_data, metadata_json, index_size, vector_count
                    ) VALUES (%s, %s, %s::jsonb, %s, %s)
                    ON CONFLICT (index_name)
                    DO UPDATE SET
                        index_data = EXCLUDED.index_data,
                        metadata_json = EXCLUDED.metadata_json,
                        index_size = EXCLUDED.index_size,
                        vector_count = EXCLUDED.vector_count,
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    'base',
                    index_data_bytes,
                    json.dumps(metadata_json, allow_nan=False),
                    index_size,
                    vector_count
                ))
            
            print(f"âœ… DBì— ë²¡í„° ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ ({vector_count}ê°œ ë²¡í„°, {index_size:,} bytes)")
            
        except Exception as e:
            print(f"âš ï¸ DB ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def add_example(
        self,
        ocr_text: str,
        answer_json: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None,
        skip_duplicate: bool = True
    ) -> Optional[str]:
        """
        ì˜ˆì œ ì¶”ê°€ (OCR í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©)
        
        Args:
            ocr_text: OCR ì¶”ì¶œ ê²°ê³¼ í…ìŠ¤íŠ¸ (ì„ë² ë”© ëŒ€ìƒ)
            answer_json: ì •ë‹µ JSON ë”•ì…”ë„ˆë¦¬ (payloadì— ì €ì¥)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì˜ˆ: pdf_name, page_num ë“±)
            skip_duplicate: ì¤‘ë³µ ì²´í¬ ì—¬ë¶€ (Trueë©´ ê°™ì€ pdf_name+page_numì´ ìˆìœ¼ë©´ ìŠ¤í‚µ)
            
        Returns:
            ì¶”ê°€ëœ ë¬¸ì„œì˜ ID (ì¤‘ë³µì´ë©´ None)
        """
        import uuid
        
        metadata = metadata or {}
        
        # ì¤‘ë³µ ì²´í¬ (pdf_nameê³¼ page_numìœ¼ë¡œ í™•ì¸)
        if skip_duplicate:
            pdf_name = metadata.get('pdf_name')
            page_num = metadata.get('page_num')
            
            if pdf_name is not None and page_num is not None:
                # ê¸°ì¡´ ì˜ˆì œ ì¤‘ ê°™ì€ pdf_nameê³¼ page_numì´ ìˆëŠ”ì§€ í™•ì¸
                for existing_id, existing_data in self.metadata.items():
                    existing_metadata = existing_data.get('metadata', {})
                    if (existing_metadata.get('pdf_name') == pdf_name and 
                        existing_metadata.get('page_num') == page_num):
                        # ì¤‘ë³µ ë°œê²¬ - ê¸°ì¡´ ID ë°˜í™˜
                        return None
        
        # ë¬¸ì„œ ID ìƒì„±
        doc_id = str(uuid.uuid4())
        
        # ì„ë² ë”© ìƒì„±
        model = self._get_embedding_model()
        processed_text = self.preprocess_ocr_text(ocr_text)
        embedding = model.encode([processed_text], convert_to_numpy=True).astype('float32')
        
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        faiss_index = self.index.ntotal
        self.index.add(embedding)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata[doc_id] = {
            "ocr_text": ocr_text,
            "answer_json": answer_json,
            "metadata": metadata
        }
        self.id_to_index[doc_id] = faiss_index
        self.index_to_id[faiss_index] = doc_id
        
        # ì €ì¥
        self._save_index()
        
        # BM25 ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
        self._refresh_bm25_index()
        
        return doc_id
    
    def get_all_examples(self) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  ì˜ˆì œ ì¡°íšŒ
        
        Returns:
            ì˜ˆì œ ë¦¬ìŠ¤íŠ¸
        """
        examples = []
        for doc_id, data in self.metadata.items():
            examples.append({
                "id": doc_id,
                "ocr_text": data.get("ocr_text", ""),
                "answer_json": data.get("answer_json", {}),
                "metadata": data.get("metadata", {})
            })
        return examples
    
    def count_examples(self) -> int:
        """
        ë²¡í„° DBì— ì €ì¥ëœ ì˜ˆì œ ìˆ˜ ë°˜í™˜
        
        Returns:
            ì˜ˆì œ ìˆ˜
        """
        if self.use_db:
            # DB ëª¨ë“œ: DBì—ì„œ ì§ì ‘ í™•ì¸ (base ìš°ì„ , ì—†ìœ¼ë©´ ëª¨ë“  ì¸ë±ìŠ¤ í•©ì‚°)
            try:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    # ë¨¼ì € base ì¸ë±ìŠ¤ í™•ì¸
                    cursor.execute("""
                        SELECT vector_count
                        FROM rag_vector_index
                        WHERE index_name = 'base'
                        ORDER BY updated_at DESC
                        LIMIT 1
                    """)
                    row = cursor.fetchone()
                    if row and row[0]:
                        return row[0]
                    
                    # baseê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì¸ë±ìŠ¤ì˜ ë²¡í„° ìˆ˜ í•©ì‚° (shard í¬í•¨)
                    cursor.execute("""
                        SELECT COALESCE(SUM(vector_count), 0)
                        FROM rag_vector_index
                    """)
                    row = cursor.fetchone()
                    if row and row[0]:
                        return row[0]
                    
                    # DBì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë©”íƒ€ë°ì´í„° ê¸¸ì´ ë°˜í™˜
                    return len(self.metadata)
            except Exception as e:
                print(f"âš ï¸ DBì—ì„œ ë²¡í„° ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
                return len(self.metadata)
        else:
            # íŒŒì¼ ëª¨ë“œ: ë©”íƒ€ë°ì´í„° ê¸¸ì´ ë°˜í™˜
            return len(self.metadata)
    
    # ============================================
    # OCR í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
    # ============================================
    
    @staticmethod
    def preprocess_ocr_text(ocr_text: str) -> str:
        """
        ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ OCR í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            ocr_text: ì›ë³¸ OCR í…ìŠ¤íŠ¸
            
        Returns:
            ì „ì²˜ë¦¬ëœ OCR í…ìŠ¤íŠ¸
        """
        import re
        
        # 1. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', ocr_text)
        
        # 2. ìˆ«ì ì •ê·œí™” (ì˜ˆ: "1,234" -> "1234")
        text = re.sub(r'(\d+),(\d+)', r'\1\2', text)
        
        # 3. ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n+', ' ', text)
        
        return text.strip()
    
    @staticmethod
    def _tokenize(text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í°í™” (ì¼ë³¸ì–´/í•œêµ­ì–´/ì˜ì–´ í˜¼í•© ë¬¸ì„œìš©)
        
        Args:
            text: í† í°í™”í•  í…ìŠ¤íŠ¸
            
        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        import re
        # ìˆ«ì, ì¼ë³¸ì–´, í•œêµ­ì–´, ì˜ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” í† í°í™”
        tokens = re.findall(r'\b\w+\b|[ê°€-í£]+|[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]+|[ä¸€-é¾ ]+', text)
        return tokens
    
    # ============================================
    # BM25 ì¸ë±ìŠ¤ ê´€ë¦¬
    # ============================================
    
    def _build_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (ì§€ì—° ë¡œë”©)"""
        try:
            from rank_bm25 import BM25Okapi
        except ImportError:
            self._bm25_index = None
            return
        
        if self._bm25_index is not None:
            return  # ì´ë¯¸ êµ¬ì¶•ë¨
        
        all_examples = self.get_all_examples()
        
        if not all_examples:
            self._bm25_index = None
            return
        
        # OCR í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
        self._bm25_texts = []
        self._bm25_example_map = {}
        
        for example in all_examples:
            ocr_text = example.get("ocr_text", "")
            doc_id = example.get("id", "")
            
            if not doc_id:
                continue
            
            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ í† í°í™”
            preprocessed = self.preprocess_ocr_text(ocr_text)
            tokens = self._tokenize(preprocessed)
            
            if tokens:  # í† í°ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                self._bm25_texts.append(tokens)
                self._bm25_example_map[doc_id] = len(self._bm25_texts) - 1
        
        # BM25 ì¸ë±ìŠ¤ ìƒì„±
        if self._bm25_texts:
            self._bm25_index = BM25Okapi(self._bm25_texts)
        else:
            self._bm25_index = None
    
    def _refresh_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨ (ì˜ˆì œ ì¶”ê°€/ì‚­ì œ í›„ í˜¸ì¶œ)"""
        self._bm25_index = None
        self._bm25_texts = None
        self._bm25_example_map = None
        self._build_bm25_index()
    
    # ============================================
    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹
    # ============================================
    
    def _create_search_result(
        self,
        doc_id: str,
        data: Dict[str, Any],
        similarity: float,
        distance: float,
        source: str
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„± (í—¬í¼ ë©”ì„œë“œ)
        
        Args:
            doc_id: ë¬¸ì„œ ID
            data: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            similarity: ìœ ì‚¬ë„ ì ìˆ˜
            distance: ê±°ë¦¬ ì ìˆ˜
            source: ì¶œì²˜ ("base" ë˜ëŠ” "shard")
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "ocr_text": data.get("ocr_text", ""),
            "answer_json": data.get("answer_json", {}),
            "metadata": data.get("metadata", {}),
            "similarity": similarity,
            "distance": float(distance),
            "id": doc_id,
            "source": source
        }
    
    def _normalize_score(self, score: float, min_score: float, max_score: float) -> float:
        """
        ì ìˆ˜ ì •ê·œí™” (í—¬í¼ ë©”ì„œë“œ)
        
        Args:
            score: ì •ê·œí™”í•  ì ìˆ˜
            min_score: ìµœì†Œ ì ìˆ˜
            max_score: ìµœëŒ€ ì ìˆ˜
            
        Returns:
            ì •ê·œí™”ëœ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if max_score > min_score:
            return (score - min_score) / (max_score - min_score)
        elif max_score == min_score and max_score > 0:
            return 1.0
        else:
            return 0.0
    
    def search_vector_only(
        self,
        query_text: str,
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ (base ì¸ë±ìŠ¤ë§Œ ê²€ìƒ‰)
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (OCR í…ìŠ¤íŠ¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì „ì²˜ë¦¬ ì ìš©
        processed_query = self.preprocess_ocr_text(query_text)
        
        # ì„ë² ë”© ìƒì„±
        model = self._get_embedding_model()
        query_embedding = model.encode([processed_query], convert_to_numpy=True).astype('float32')
        
        all_results = []
        
        # base ì¸ë±ìŠ¤ ê²€ìƒ‰
        if self.index is None:
            print(f"âš ï¸ RAG ê²€ìƒ‰: ì¸ë±ìŠ¤ê°€ Noneì…ë‹ˆë‹¤. ë²¡í„° DBê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return []
        
        if self.index.ntotal > 0:
            k = min(top_k * 2, self.index.ntotal)
            distances, indices = self.index.search(query_embedding, k)
            
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx == -1:
                    continue
                
                doc_id = self.index_to_id.get(idx)
                if not doc_id:
                    continue
                
                similarity = max(0.0, 1.0 - (distance / 100.0))
                if similarity < similarity_threshold:
                    continue
                
                data = self.metadata.get(doc_id, {})
                # deleted ìƒíƒœ í•„í„°ë§
                page_metadata = data.get("metadata", {})
                if page_metadata.get("status") == "deleted":
                    continue
                
                all_results.append(self._create_search_result(doc_id, data, similarity, distance, "base"))
        else:
            print(f"âš ï¸ RAG ê²€ìƒ‰: ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ntotal={self.index.ntotal}, ë©”íƒ€ë°ì´í„°={len(self.metadata)}ê°œ)")
        
        # ìœ ì‚¬ë„ë¡œ ì •ë ¬ ë° ì¤‘ë³µ ì œê±° (doc_id ê¸°ì¤€)
        seen_doc_ids = set()
        unique_results = []
        for result in sorted(all_results, key=lambda x: x["similarity"], reverse=True):
            doc_id = result["id"]
            if doc_id not in seen_doc_ids:
                seen_doc_ids.add(doc_id)
                unique_results.append(result)
        
        return unique_results[:top_k]
    
    def search_hybrid(
        self,
        query_text: str,
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        hybrid_alpha: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 + ë²¡í„° ê²€ìƒ‰ ê²°í•©
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (OCR í…ìŠ¤íŠ¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
            hybrid_alpha: í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ (0.0~1.0, 0.5 = ë²¡í„°ì™€ BM25 ë™ì¼ ê°€ì¤‘ì¹˜)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (hybrid_score í¬í•¨)
        """
        # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self._build_bm25_index()
        
        if self._bm25_index is None:
            # BM25ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•˜ë©´ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
            return self.search_vector_only(query_text, top_k, similarity_threshold)
        
        # ë²¡í„° ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´)
        vector_results = self.search_vector_only(
            query_text, top_k * 3, 0.0  # threshold ë¬´ì‹œ
        )
        
        # BM25 ê²€ìƒ‰
        processed_query = self.preprocess_ocr_text(query_text)
        query_tokens = self._tokenize(processed_query)
        
        if not query_tokens:
            return self.search_vector_only(query_text, top_k, similarity_threshold)
        
        bm25_scores_list = self._bm25_index.get_scores(query_tokens)
        
        # doc_id -> BM25 ì ìˆ˜ ë§¤í•‘
        bm25_scores = {}
        for doc_id, bm25_idx in self._bm25_example_map.items():
            if bm25_idx < len(bm25_scores_list):
                bm25_scores[doc_id] = bm25_scores_list[bm25_idx]
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        hybrid_results = []
        candidate_bm25_scores = [bm25_scores.get(r["id"], 0.0) for r in vector_results]
        
        if candidate_bm25_scores:
            max_bm25 = max(candidate_bm25_scores)
            min_bm25 = min(candidate_bm25_scores)
        else:
            max_bm25 = 1.0
            min_bm25 = 0.0
        
        for result in vector_results:
            doc_id = result["id"]
            vector_similarity = result["similarity"]
            
            # BM25 ì ìˆ˜ ì •ê·œí™”
            bm25_score = bm25_scores.get(doc_id, 0.0)
            normalized_bm25 = self._normalize_score(bm25_score, min_bm25, max_bm25)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
            hybrid_score = hybrid_alpha * vector_similarity + (1 - hybrid_alpha) * normalized_bm25
            
            # ë²¡í„° ìœ ì‚¬ë„ê°€ thresholdë¥¼ í†µê³¼í•˜ë©´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ì™€ ê´€ê³„ì—†ì´ í¬í•¨
            # (BM25 ì ìˆ˜ê°€ ë‚®ì•„ë„ ë²¡í„° ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ìœ ì§€)
            if hybrid_score < similarity_threshold and vector_similarity < similarity_threshold:
                continue
            
            result["bm25_score"] = normalized_bm25
            result["hybrid_score"] = hybrid_score
            hybrid_results.append(result)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
        
        return hybrid_results[:top_k]
    
    # ============================================
    # Shard ê´€ë¦¬ ë©”ì„œë“œ
    # ============================================
    
    def build_shard(
        self,
        pages: List[Dict[str, Any]]
    ) -> Optional[Tuple[str, str]]:
        """
        ìƒˆë¡œìš´ shard FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            pages: í˜ì´ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                ê° í˜ì´ì§€ëŠ” {
                    'pdf_name': str,
                    'page_num': int,
                    'ocr_text': str,
                    'answer_json': Dict[str, Any],
                    'metadata': Dict[str, Any],
                    'page_key': str,
                    'page_hash': str
                } í˜•íƒœ
        
        Returns:
            (shard_path, shard_id) íŠœí”Œ (ì‹¤íŒ¨ ì‹œ None)
        """
        if not pages:
            return None
        
        import uuid
        from datetime import datetime
        
        # shard íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        shard_id = str(uuid.uuid4())[:8]
        shard_filename = f"shard_{timestamp}_{shard_id}.faiss"
        shard_path = os.path.join(self.shards_dir, shard_filename)
        shard_metadata_path = shard_path.replace(".faiss", "_metadata.json")
        
        try:
            # ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            model = self._get_embedding_model()
            embedding_dim = self._get_embedding_dim()
            
            # ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±
            shard_index = faiss.IndexFlatL2(embedding_dim)
            
            # shard ë©”íƒ€ë°ì´í„°
            shard_metadata = {}
            shard_id_to_index = {}
            shard_index_to_id = {}
            
            # ê° í˜ì´ì§€ ì„ë² ë”© ë° ì¶”ê°€
            for page_data in pages:
                ocr_text = page_data.get('ocr_text', '')
                answer_json = page_data.get('answer_json', {})
                base_metadata = page_data.get('metadata', {})
                page_key = page_data.get('page_key', '')
                page_hash = page_data.get('page_hash', '')
                
                if not ocr_text:
                    continue
                
                # ë¬¸ì„œ ID ìƒì„±
                doc_id = str(uuid.uuid4())
                
                # ì„ë² ë”© ìƒì„±
                processed_text = self.preprocess_ocr_text(ocr_text)
                embedding = model.encode([processed_text], convert_to_numpy=True).astype('float32')
                
                # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
                faiss_index = shard_index.ntotal
                shard_index.add(embedding)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥ (page ì‹ë³„ ì •ë³´ í•„ìˆ˜ í¬í•¨)
                shard_metadata[doc_id] = {
                    "ocr_text": ocr_text,
                    "answer_json": answer_json,
                    "metadata": {
                        **base_metadata,
                        "page_key": page_key,
                        "page_hash": page_hash,
                        "shard_id": shard_id,
                        "status": "staged"
                    }
                }
                shard_id_to_index[doc_id] = faiss_index
                shard_index_to_id[faiss_index] = doc_id
            
            if shard_index.ntotal == 0:
                print("âš ï¸ shardì— ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # shard ì¸ë±ìŠ¤ ì €ì¥
            if self.use_db:
                # DBì— ì €ì¥
                shard_index_name = f"shard_{shard_id}"
                # serialize_indexëŠ” bytesë¥¼ ë°˜í™˜í•˜ì§€ë§Œ, psycopg2 í˜¸í™˜ì„ ìœ„í•´ ëª…ì‹œì  ë³€í™˜
                serialized = faiss.serialize_index(shard_index)
                if hasattr(serialized, 'tobytes'):
                    index_data_bytes = serialized.tobytes()
                else:
                    index_data_bytes = bytes(serialized)
                index_size = len(index_data_bytes)
                vector_count = shard_index.ntotal
                
                # NaN ê°’ ì²˜ë¦¬
                def clean_for_json(obj):
                    """NaN, Infinity ê°’ì„ nullë¡œ ë³€í™˜"""
                    import math
                    if isinstance(obj, dict):
                        return {k: clean_for_json(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [clean_for_json(item) for item in obj]
                    elif isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    return obj
                
                cleaned_shard_metadata = clean_for_json(shard_metadata)
                shard_data = {
                    "metadata": cleaned_shard_metadata,
                    "id_to_index": shard_id_to_index,
                    "index_to_id": {str(k): v for k, v in shard_index_to_id.items()},
                    "shard_id": shard_id
                }
                
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        INSERT INTO rag_vector_index (
                            index_name, index_data, metadata_json, index_size, vector_count
                        ) VALUES (%s, %s, %s::jsonb, %s, %s)
                    """, (
                        shard_index_name,
                        index_data_bytes,
                        json.dumps(shard_data, allow_nan=False),
                        index_size,
                        vector_count
                    ))
                
                print(f"âœ… Shard ìƒì„± ì™„ë£Œ (DB ì €ì¥): {shard_index_name} ({vector_count}ê°œ ë²¡í„°)")
                return (shard_index_name, shard_id)  # DB ëª¨ë“œì—ì„œëŠ” index_name ë°˜í™˜
            else:
                # íŒŒì¼ì— ì €ì¥ (ê¸°ì¡´ ë°©ì‹)
                faiss.write_index(shard_index, shard_path)
                
                shard_data = {
                    "metadata": shard_metadata,
                    "id_to_index": shard_id_to_index,
                    "index_to_id": shard_index_to_id,
                    "shard_id": shard_id
                }
                with open(shard_metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(shard_data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… Shard ìƒì„± ì™„ë£Œ: {shard_filename} ({shard_index.ntotal}ê°œ ë²¡í„°)")
                return (shard_path, shard_id)
            
        except Exception as e:
            print(f"âŒ Shard ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ìƒì„±ëœ íŒŒì¼ ì •ë¦¬
            if os.path.exists(shard_path):
                try:
                    os.remove(shard_path)
                except:
                    pass
            if os.path.exists(shard_metadata_path):
                try:
                    os.remove(shard_metadata_path)
                except:
                    pass
            return None
    
    def merge_shard(self, shard_path: str) -> bool:
        """
        shardë¥¼ base ì¸ë±ìŠ¤ì— ì›ìì ìœ¼ë¡œ mergeí•©ë‹ˆë‹¤.
        
        Args:
            shard_path: shard FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” DB index_name
            
        Returns:
            merge ì„±ê³µ ì—¬ë¶€
        """
        if self.use_db:
            # DB ëª¨ë“œ: shard_pathëŠ” index_name
            shard_index_name = shard_path
            try:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT index_data, metadata_json
                        FROM rag_vector_index
                        WHERE index_name = %s
                    """, (shard_index_name,))
                    
                    row = cursor.fetchone()
                    if not row:
                        print(f"âŒ DBì—ì„œ Shardë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {shard_index_name}")
                        return False
                    
                    shard_index_data = row[0]
                    shard_metadata_json = row[1]
                    
                    # BYTEAë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (faiss.deserialize_indexëŠ” numpy ë°°ì—´ í•„ìš”)
                    if isinstance(shard_index_data, memoryview):
                        shard_index_data = np.frombuffer(shard_index_data, dtype=np.uint8)
                    elif isinstance(shard_index_data, bytes):
                        shard_index_data = np.frombuffer(shard_index_data, dtype=np.uint8)
                    else:
                        shard_index_data = np.frombuffer(bytes(shard_index_data), dtype=np.uint8)
                    
                    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                    shard_index = faiss.deserialize_index(shard_index_data)
                    
                    # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                    shard_data = shard_metadata_json
                    shard_metadata = shard_data.get('metadata', {})
                    shard_id_to_index = shard_data.get('id_to_index', {})
                    shard_index_to_id_raw = shard_data.get('index_to_id', {})
                    shard_index_to_id = {int(k): v for k, v in shard_index_to_id_raw.items()}
            except Exception as e:
                print(f"âŒ DBì—ì„œ Shard ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        else:
            # íŒŒì¼ ëª¨ë“œ
            if not os.path.exists(shard_path):
                print(f"âŒ Shard íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {shard_path}")
                return False
            
            shard_metadata_path = shard_path.replace(".faiss", "_metadata.json")
            if not os.path.exists(shard_metadata_path):
                print(f"âŒ Shard ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {shard_metadata_path}")
                return False
            
            # shard ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
            shard_index = faiss.read_index(shard_path)
            shard_metadata, shard_id_to_index, shard_index_to_id = self._load_metadata_from_file(shard_metadata_path)
        
        try:
            # base ì¸ë±ìŠ¤ ë¡œë“œ (mergeí•˜ë ¤ëŠ” shard ì œì™¸)
            if self.use_db:
                # mergeí•˜ë ¤ëŠ” shardë¥¼ ì œì™¸í•˜ê³  base ë¡œë“œ
                base_index, base_metadata, base_id_to_index, base_index_to_id = self._load_index_from_db(exclude_shard_name=shard_index_name)
                if base_index is None:
                    embedding_dim = self._get_embedding_dim()
                    base_index = faiss.IndexFlatL2(embedding_dim)
                    base_metadata = {}
                    base_id_to_index = {}
                    base_index_to_id = {}
            else:
                if os.path.exists(self.base_index_path):
                    base_index = faiss.read_index(self.base_index_path)
                    base_metadata, base_id_to_index, base_index_to_id = self._load_metadata_from_file(self.base_metadata_path)
                else:
                    embedding_dim = self._get_embedding_dim()
                    base_index = faiss.IndexFlatL2(embedding_dim)
                    base_metadata = {}
                    base_id_to_index = {}
                    base_index_to_id = {}
            
            # baseì˜ í˜„ì¬ ë²¡í„° ìˆ˜ (merge ì „)
            base_vector_count = base_index.ntotal
            
            # FAISS ì¸ë±ìŠ¤ merge
            base_index.merge_from(shard_index)
            
            # ë©”íƒ€ë°ì´í„° merge (ì¸ë±ìŠ¤ ì˜¤í”„ì…‹ ì¡°ì •, status ì—…ë°ì´íŠ¸)
            for doc_id, shard_faiss_idx in shard_id_to_index.items():
                new_faiss_idx = base_vector_count + shard_faiss_idx
                page_metadata = shard_metadata[doc_id].copy()
                # statusë¥¼ mergedë¡œ ì—…ë°ì´íŠ¸
                if "metadata" in page_metadata:
                    page_metadata["metadata"]["status"] = "merged"
                
                base_metadata[doc_id] = page_metadata
                base_id_to_index[doc_id] = new_faiss_idx
                base_index_to_id[new_faiss_idx] = doc_id
            
            # ì €ì¥ (DB ë˜ëŠ” íŒŒì¼)
            if self.use_db:
                # DBì— ì €ì¥
                self.index = base_index
                self.metadata = base_metadata
                self.id_to_index = base_id_to_index
                self.index_to_id = base_index_to_id
                self._save_index_to_db()  # DBì— ì €ì¥
                
                # merge ì™„ë£Œ í›„ shard ì‚­ì œ (DBì—ì„œ)
                try:
                    with self.db.get_connection() as conn:
                        cursor = conn.cursor()
                        cursor.execute("""
                            DELETE FROM rag_vector_index
                            WHERE index_name = %s
                        """, (shard_index_name,))
                    print(f"âœ… Shard ì‚­ì œ ì™„ë£Œ (DB): {shard_index_name}")
                except Exception as e:
                    print(f"âš ï¸ Shard ì‚­ì œ ì‹¤íŒ¨: {e}")
            else:
                # íŒŒì¼ì— ì €ì¥ (ì›ìì  write)
                tmp_index_path = self.base_index_path + ".tmp"
                tmp_metadata_path = self.base_metadata_path + ".tmp"
                
                faiss.write_index(base_index, tmp_index_path)
                
                base_data = {
                    "metadata": base_metadata,
                    "id_to_index": base_id_to_index,
                    "index_to_id": base_index_to_id
                }
                with open(tmp_metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(base_data, f, ensure_ascii=False, indent=2)
                    f.flush()
                    os.fsync(f.fileno())  # ë””ìŠ¤í¬ ë™ê¸°í™”
                
                # ì›ìì  rename (ì„ì‹œ íŒŒì¼ â†’ ì‹¤ì œ íŒŒì¼)
                os.rename(tmp_index_path, self.base_index_path)
                os.rename(tmp_metadata_path, self.base_metadata_path)
                
                # ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ ê°±ì‹ 
                self.index = base_index
                self.metadata = base_metadata
                self.id_to_index = base_id_to_index
                self.index_to_id = base_index_to_id
                
                # merge ì™„ë£Œ í›„ shard íŒŒì¼ ì‚­ì œ
                try:
                    if os.path.exists(shard_path):
                        os.remove(shard_path)
                    if os.path.exists(shard_metadata_path):
                        os.remove(shard_metadata_path)
                    print(f"âœ… Shard íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {shard_path}")
                except Exception as e:
                    print(f"âš ï¸ Shard íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            # BM25 ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
            self._refresh_bm25_index()
            
            print(f"âœ… Shard merge ì™„ë£Œ: {shard_index.ntotal}ê°œ ë²¡í„°ê°€ baseì— ì¶”ê°€ë¨")
            return True
            
        except Exception as e:
            print(f"âŒ Shard merge ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (íŒŒì¼ ëª¨ë“œì¼ ë•Œë§Œ)
            if not self.use_db:
                for tmp_path in [tmp_index_path, tmp_metadata_path]:
                    if os.path.exists(tmp_path):
                        try:
                            os.remove(tmp_path)
                        except:
                            pass
            return False
    
    def get_shard_paths(self) -> List[str]:
        """
        ëª¨ë“  shard íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        
        Returns:
            shard íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        if not os.path.exists(self.shards_dir):
            return []
        
        shard_paths = []
        for filename in os.listdir(self.shards_dir):
            if filename.endswith(".faiss"):
                shard_paths.append(os.path.join(self.shards_dir, filename))
        
        return sorted(shard_paths)
    
    def search_similar_advanced(
        self,
        query_text: str,
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        search_method: str = "vector",  # "vector", "hybrid"
        hybrid_alpha: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        í†µí•© ê²€ìƒ‰ í•¨ìˆ˜ (ê²€ìƒ‰ ë°©ì‹ ì„ íƒ ê°€ëŠ¥)
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (OCR í…ìŠ¤íŠ¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
            search_method: ê²€ìƒ‰ ë°©ì‹ ("vector", "hybrid")
            hybrid_alpha: í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ (hybrid ë°©ì‹ ì‚¬ìš© ì‹œ)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if search_method == "hybrid":
            return self.search_hybrid(
                query_text, top_k, similarity_threshold, hybrid_alpha
            )
        else:  # "vector" ë˜ëŠ” ê¸°ë³¸ê°’
            return self.search_vector_only(
                query_text, top_k, similarity_threshold
            )


# ì „ì—­ RAG Manager ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_rag_manager: Optional[RAGManager] = None
_rag_manager_lock = Lock()  # ì‹±ê¸€í†¤ ìƒì„± ë½


def get_rag_manager(use_db: bool = True) -> RAGManager:
    """
    ì „ì—­ RAG Manager ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)
    
    Args:
        use_db: Trueë©´ DBì— ì €ì¥, Falseë©´ ë¡œì»¬ íŒŒì¼ì— ì €ì¥ (ê¸°ë³¸ê°’: True)
    
    Returns:
        RAGManager ì¸ìŠ¤í„´ìŠ¤
    """
    global _rag_manager
    if _rag_manager is None:
        with _rag_manager_lock:
            # ì´ì¤‘ ì²´í¬
            if _rag_manager is None:
                _rag_manager = RAGManager(use_db=use_db)
    return _rag_manager
